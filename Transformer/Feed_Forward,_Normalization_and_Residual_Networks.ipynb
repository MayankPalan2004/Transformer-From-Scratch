{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fIzTJYWTGrjb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self,num_layers, d,h):\n",
        "        self.d = d\n",
        "        self.h = h\n",
        "        self.d_k = d/ h\n",
        "        self.v_weights, self.k_weights, self.q_weights, self.lin_trans = (np.random.randn(4,d,d))\n",
        "        self.bias_weights1 = np.zeros(d)\n",
        "        self.bias_weights1 = np.zeros(d)\n",
        "\n",
        "\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.reshape(batch_size,-1,self.h,self.d_k)\n",
        "        return x.transpose(0,2,1,3)\n",
        "    def scaled_self_attention(self,queries,keys,values):\n",
        "        attention_scores = np.matmul(queries, keys.transpose(0,2,1)) / np.sqrt(self.d_k)\n",
        "        attention_weights = softmax(attention_scores)\n",
        "        attention_outputs = np.matmul(attention_weights, values)\n",
        "        return attention_output\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        batch_size, seq_len,d = x.shape\n",
        "\n",
        "        values = np.dot(x, self.v_weights)\n",
        "        keys = np.dot(x, self.k_weights)\n",
        "        queries = np.dot(x, self.q_weights)\n",
        "\n",
        "        values = self.split_heads(values, batch_size)\n",
        "        keys = self.split_heads(keys, batch_size)\n",
        "        queries = self.split_heads(queries, batch_size)\n",
        "\n",
        "        attention_output=self.scaled_self_attention(self,queries,keys,values)\n",
        "        attention_output = attention_outputs.reshape(batch_size,-1)\n",
        "        normalized_output = self.layer_norm(x + attention_output)\n",
        "        ffn_output = np.dot(normalized_output, self.lin_trans) + self.bias_weights1\n",
        "        ffn_output = np.maximum(0, ffn_output)\n",
        "        output = np.dot(ffn_output, self.lin_trans.T) + self.bias_weights2\n",
        "\n",
        "        return output\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def backward(self, grad_output):\n",
        "        grad_bias2 = np.sum(grad_output, axis=0)\n",
        "        grad_ffn_output = np.dot(grad_output, self.lin_trans.T)\n",
        "        grad_ffn_output[ffn_output <= 0] = 0\n",
        "        grad_lin_trans = np.dot(attention_output.T, grad_ffn_output)\n",
        "        grad_bias1 = np.sum(grad_ffn_output, axis=0)\n",
        "\n",
        "        grad_attention_output = np.dot(grad_ffn_output, self.lin_trans.T)\n",
        "        grad_attention_output = grad_attention_output.reshape(batch_size, self.h, -1, self.d_k)\n",
        "\n",
        "        grad_attention_weights = np.matmul(grad_attention_output, values.transpose(0, 2, 1))\n",
        "        grad_values = np.matmul(grad_attention_output.transpose(0, 2, 1, 3), attention_weights)\n",
        "        grad_keys = np.matmul(queries.transpose(0, 2, 1, 3).transpose(1, 2, 0, 3), grad_attention_output)\n",
        "\n",
        "        grad_values = grad_values.reshape(batch_size, -1, self.d)\n",
        "        grad_keys = grad_keys.reshape(batch_size, -1, self.d)\n",
        "        grad_queries = np.matmul(grad_attention_weights, keys)\n",
        "\n",
        "        grad_v_weights = np.dot(x.T, grad_values)\n",
        "        grad_k_weights = np.dot(x.T, grad_keys)\n",
        "        grad_q_weights = np.dot(x.T, grad_queries)\n",
        "\n",
        "        self.v_weights -= self.learning_rate * grad_v_weights\n",
        "        self.k_weights -= self.learning_rate * grad_k_weights\n",
        "        self.q_weights -= self.learning_rate * grad_q_weights\n",
        "        self.lin_trans -= self.learning_rate * grad_lin_trans\n",
        "        self.bias_weights1 -= self.learning_rate * grad_bias1\n",
        "        self.bias_weights2 -= self.learning_rate * grad_bias2\n",
        "\n",
        "        return grad_values"
      ],
      "metadata": {
        "id": "vIzNFymUcg6g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 6 #cloning these layers for N=6 repeating the same layers again\n",
        "d = 512\n",
        "h = 8\n",
        "MultiHeadAttention(num_layers,d,h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og_6WUpJcrEd",
        "outputId": "0d87ea9d-a57f-4f55-fbd5-6a7bde94cbc5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MultiHeadAttention at 0x7a9d8119f160>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization:\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = np.ones(d)  # Scaling parameter\n",
        "        self.beta = np.zeros(d)   # Shifting parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        variance = np.var(x, axis=-1, keepdims=True)\n",
        "        normalized_x = (x - mean) / np.sqrt(variance + self.epsilon)\n",
        "        out = self.gamma * normalized_x + self.beta\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_normalized_x = grad_output * self.gamma\n",
        "        grad_variance = np.sum(grad_normalized_x * (self.x - np.mean(self.x, axis=-1, keepdims=True)), axis=-1, keepdims=True) * -0.5 * ((self.variance + self.epsilon) ** -1.5)\n",
        "        grad_mean = np.sum(grad_normalized_x * -1 / np.sqrt(self.variance + self.epsilon), axis=-1, keepdims=True) + grad_variance * np.mean(self.x, axis=-1, keepdims=True) * -2 / d\n",
        "        grad_input = grad_normalized_x / np.sqrt(self.variance + self.epsilon) + grad_variance * 2 * (self.x - np.mean(self.x, axis=-1, keepdims=True)) / d + grad_mean / d\n",
        "        grad_gamma = np.sum(grad_output * (self.x - np.mean(self.x, axis=-1, keepdims=True)) / np.sqrt(self.variance + self.epsilon), axis=-1)\n",
        "        grad_beta = np.sum(grad_output, axis=-1)\n",
        "        self.gamma -= self.learning_rate * grad_gamma\n",
        "        self.beta -= self.learning_rate * grad_beta\n",
        "        return grad_input\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, num_layers, d, h):\n",
        "        self.d = d\n",
        "        self.h = h\n",
        "        self.d_k = d // h\n",
        "        self.v_weights, self.k_weights, self.q_weights, self.lin_trans = (np.random.randn(4, d, d))\n",
        "        self.bias_weights1, self.bias_weights2 = (np.zeros(d), np.zeros(d))\n",
        "        self.layers = [LayerNormalization() for _ in range(num_layers)]"
      ],
      "metadata": {
        "id": "u6UR3YBsfsb1"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}