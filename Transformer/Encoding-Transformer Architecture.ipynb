{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ewsgw68IXYPv"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))"
      ],
      "metadata": {
        "id": "Exv6cCZ-Xcpl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    matmul_qk = np.dot(query, key.T)\n",
        "    d_k = key.shape[-1]\n",
        "    scaled_attention_logits = matmul_qk / np.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    attention_weights = softmax(scaled_attention_logits, axis=-1)\n",
        "    output = np.dot(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "qWudTMt2XnL-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x, axis=-1):\n",
        "    e_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=axis, keepdims=True)"
      ],
      "metadata": {
        "id": "7pRfgS1dXs0H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding:\n",
        "    def __init__(self, d_model, max_seq_len=512):\n",
        "        self.encoding = self.generate_positional_encoding(d_model, max_seq_len)\n",
        "\n",
        "    def generate_positional_encoding(self, d_model, max_seq_len):\n",
        "        position = np.arange(max_seq_len).reshape(-1, 1)\n",
        "        div_term = 1 / np.power(10000, 2 * (np.arange(d_model) // 2) / d_model)\n",
        "        encoding = np.zeros((max_seq_len, d_model))\n",
        "\n",
        "        encoding[:, 0::2] = np.sin(position * div_term[0::2])\n",
        "        encoding[:, 1::2] = np.cos(position * div_term[1::2])\n",
        "\n",
        "        return encoding\n",
        "\n",
        "    def get_positional_encoding(self, seq_len):\n",
        "        return self.encoding[:seq_len]\n"
      ],
      "metadata": {
        "id": "1-iiSvrPX0Cn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder:\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, max_seq_len):\n",
        "        self.num_layers = num_layers\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dff = dff\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, dff)\n",
        "        self.layers = self.create_encoder_layers()\n",
        "\n",
        "    def create_encoder_layers(self):\n",
        "        layers = []\n",
        "        for _ in range(self.num_layers):\n",
        "            layers.append(self.create_encoder_layer())\n",
        "        return layers\n",
        "\n",
        "    def create_encoder_layer(self):\n",
        "        return lambda x, mask: self.encoder_layer(x, mask)\n",
        "\n",
        "    def encoder_layer(self, x, mask):\n",
        "        x += self.pos_encoding.get_positional_encoding(x.shape[1])\n",
        "        x, _ = self.multi_head_attention(x, x, x, mask)\n",
        "        x = self.feed_forward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "H7QUGPckX7ai"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFNGQYdjYALO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}