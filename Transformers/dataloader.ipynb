{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpt8Hp4dmh_t",
        "outputId": "16a3a4f0-4d94-4c4d-d1ff-938ad9096783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "class NexxtWordPredictor:\n",
        "    def __init__(self, dataset_path=None, batch_size=32, max_sequence_length=50, padding_token=0, seed=42):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.padding_token = padding_token\n",
        "        self.seed = seed\n",
        "        self.input_sequences = []  # Corrected attribute name\n",
        "        self.output_sequences = []  # Corrected attribute name\n",
        "        self.source_vocab = {}  # Initialize an empty vocabulary\n",
        "\n",
        "        if dataset_path:\n",
        "            self.load_custom_dataset(dataset_path)\n",
        "        else:\n",
        "            print(\"No dataset provided. Please load a dataset using load_custom_dataset() method.\")\n",
        "\n",
        "    def load_custom_dataset(self, sentences):\n",
        "        self.sentences = [sentence.strip() for sentence in sentences]\n",
        "        self.build_vocab()\n",
        "        self.generate_input_output_sequences()  # Add this line to generate input and output sequences\n",
        "\n",
        "\n",
        "    def tokenize_sentence(self, sentence):\n",
        "        return sentence.split()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        words = []\n",
        "        for sentence in self.sentences:\n",
        "            words.extend(self.tokenize_sentence(sentence))\n",
        "\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter(words)\n",
        "\n",
        "        # Create a vocabulary with words that occur more than a certain threshold\n",
        "        min_token_freq = 1  # Adjust as needed\n",
        "        for word, count in word_counts.items():\n",
        "            if count > min_token_freq and word not in self.source_vocab:\n",
        "                self.source_vocab[word] = len(self.source_vocab)\n",
        "\n",
        "    def pad_sequence(self, sequence):\n",
        "        return sequence + [0] * (self.max_sequence_length - len(sequence))\n",
        "\n",
        "    def generate_input_output_sequences(self):\n",
        "        for sentence in self.sentences:\n",
        "            tokens = self.tokenize_sentence(sentence)\n",
        "            for i in range(1, len(tokens)):\n",
        "                input_seq = tokens[:i]  # Input sequence is the tokens up to the current position\n",
        "                output_word = tokens[i]  # Output word is the next word\n",
        "\n",
        "                # Pad input sequence to the desired max_sequence_length\n",
        "                input_seq = self.pad_sequence(input_seq)\n",
        "\n",
        "                self.input_sequences.append(input_seq)\n",
        "                self.output_sequences.append(output_word)\n",
        "\n",
        "custom_dataset = [\n",
        "    \"This is Team Transformer from scratch we have made a model from scratch .\"\n",
        "]\n",
        "\n",
        "next_word_predictor = NexxtWordPredictor(dataset_path=custom_dataset)\n",
        "#next_word_predictor = NexxtWordPredictor()\n",
        "\n",
        "# Access input_sequences and output_sequences\n",
        "print(next_word_predictor)\n",
        "input_sequences = np.array(next_word_predictor.input_sequences)\n",
        "output_sequences = np.array(next_word_predictor.output_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RqEotLNRHbH",
        "outputId": "096f17de-6ffe-4a1b-9da1-583ff60823b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.NexxtWordPredictor object at 0x7b187430ebf0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiV_kz_0UoxD",
        "outputId": "7853d94a-26a2-4d80-b047-66693304c241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
            " ['This' 'is' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' 'have' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' 'have' 'made'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' 'have' 'made'\n",
            "  'a' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' 'have' 'made'\n",
            "  'a' 'model' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' 'have' 'made'\n",
            "  'a' 'model' 'from' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0']\n",
            " ['This' 'is' 'Team' 'Transformer' 'from' 'scratch' 'we' 'have' 'made'\n",
            "  'a' 'model' 'from' 'scratch' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
            "  '0' '0' '0' '0' '0' '0' '0' '0' '0']]\n"
          ]
        }
      ]
    }
  ]
}